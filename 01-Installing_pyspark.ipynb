{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=263ec2a337ac49c39b1a9e479883d36dce83869d5b70108fe4ba30636d381a59\n",
      "  Stored in directory: c:\\users\\alchemist\\appdata\\local\\pip\\cache\\wheels\\b1\\59\\a0\\a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5ca4dd1109d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Alchemist\\anaconda3\\envs\\tflow\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyspark\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    abseil-cpp-20211102.0      |       h0e60522_0         2.3 MB  conda-forge\n",
      "    arrow-cpp-10.0.1           |   py38h3577439_0         7.1 MB\n",
      "    aws-c-common-0.4.57        |       ha925a31_1         147 KB\n",
      "    aws-c-event-stream-0.1.6   |       hd77b12b_5          26 KB\n",
      "    aws-checksums-0.1.9        |       ha925a31_0          50 KB\n",
      "    aws-sdk-cpp-1.8.185        |       hd77b12b_0         2.5 MB\n",
      "    boost-cpp-1.78.0           |       h5b4e17d_0        17.0 MB  conda-forge\n",
      "    bzip2-1.0.8                |       h8ffe710_4         149 KB  conda-forge\n",
      "    c-ares-1.18.1              |       h8ffe710_0         114 KB  conda-forge\n",
      "    ca-certificates-2022.12.7  |       h5b45459_0         143 KB  conda-forge\n",
      "    certifi-2022.12.7          |     pyhd8ed1ab_0         147 KB  conda-forge\n",
      "    gflags-2.2.2               |    ha925a31_1004          80 KB  conda-forge\n",
      "    glog-0.5.0                 |       h4797de2_0          90 KB  conda-forge\n",
      "    intel-openmp-2023.0.0      |   h57928b3_25922         2.1 MB  conda-forge\n",
      "    libblas-3.9.0              |            8_mkl         3.9 MB  conda-forge\n",
      "    libbrotlicommon-1.0.9      |       h8ffe710_7          67 KB  conda-forge\n",
      "    libbrotlidec-1.0.9         |       h8ffe710_7          33 KB  conda-forge\n",
      "    libbrotlienc-1.0.9         |       h8ffe710_7         716 KB  conda-forge\n",
      "    libcblas-3.9.0             |            8_mkl         3.9 MB  conda-forge\n",
      "    libcurl-7.88.1             |       h86230a5_0         328 KB\n",
      "    liblapack-3.9.0            |            8_mkl         3.9 MB  conda-forge\n",
      "    libprotobuf-3.20.3         |       h23ce68f_0         2.2 MB\n",
      "    libssh2-1.10.0             |       h680486a_2         227 KB  conda-forge\n",
      "    libthrift-0.15.0           |       h636ae23_0         872 KB  conda-forge\n",
      "    lz4-c-1.9.4                |       h2bbff1b_0         143 KB\n",
      "    mkl-2020.4                 |     hb70f87d_311       172.4 MB  conda-forge\n",
      "    numpy-1.22.3               |   py38h1d2777f_2         6.1 MB  conda-forge"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
